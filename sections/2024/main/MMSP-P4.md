# ICASSP-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/ICASSP-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/SPCOM-P3.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/IVMSP-P11.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Multimodal Emotion/Sentiment Analysis

![Section Papers](https://img.shields.io/badge/Section%20Papers-11-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-1-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| AttA-NET: Attention Aggregation Network for Audio-Visual Emotion Recognition | [![GitHub](https://img.shields.io/github/stars/NariFan2002/AttA-NET?style=flat)](https://github.com/NariFan2002/AttA-NET) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447640-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447640) | :heavy_minus_sign: |
| Fusing Modality-Specific Representations and Decisions for Multimodal Emotion Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447035-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447035) | :heavy_minus_sign: |
| Speaker-Centric Multimodal Fusion Networks for Emotion Recognition in Conversations | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447720-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447720) | :heavy_minus_sign: |
| CLIP-MSA: Incorporating Inter-Modal Dynamics and Common Knowledge to Multimodal Sentiment Analysis With Clip | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446825-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446825) | :heavy_minus_sign: |
| Circular Decomposition and Cross-Modal Recombination for Multimodal Sentiment Analysis | [![GitHub](https://img.shields.io/github/stars/nianhua20/GCD-CMR?style=flat)](https://github.com/nianhua20/GCD-CMR) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446166-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446166) | :heavy_minus_sign: |
| A Novel Multimodal Sentiment Analysis Model Based on Gated Fusion and Multi-Task Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446040-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446040) | :heavy_minus_sign: |
| Modality-Dependent Sentiments Exploring for Multi-Modal Sentiment Classification | [![GitHub](https://img.shields.io/github/stars/royal-dargon/MDSE?style=flat)](https://github.com/royal-dargon/MDSE) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10445820-E4A42C.svg)](https://ieeexplore.ieee.org/document/10445820) | :heavy_minus_sign: |
| Emotion-Aligned Contrastive Learning Between Images and Music | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447276-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447276) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2308.12610-b31b1b.svg)](https://arxiv.org/abs/2308.12610) | :heavy_minus_sign: |
| MMRBN: Rule-Based Network for Multimodal Emotion Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447930-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447930) | :heavy_minus_sign: |
| Inter-Modality and Intra-Sample Alignment for Multi-Modal Emotion Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446571-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446571) | :heavy_minus_sign: |
| MDAVIF: A Multi-Domain Acoustical-Visual Information Fusion Model for Depression Recognition from Vlog Data | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446491-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446491) | :heavy_minus_sign: |

